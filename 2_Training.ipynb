{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor, from_numpy, flatten, nn\n",
    "import operator\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from utils import get_data, get_data_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the KMNIST training data as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check to make sure we haven't loaded tensors in nwrong order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = X_train.shape\n",
    "# num_classes:\n",
    "c = int(y_train.max()+1)\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop:\n",
    "1. run batch through model to get predictions\n",
    "2. calculate the loss with loss(pred, actual)\n",
    "3. calculate grad of each param with backpropgation\n",
    "4. update the params\n",
    "\n",
    "For multiclass classification, the loss function we use is the cross entropy (in pytorch this is deinfed in `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use batches of data because in a normal setting the whole dataset is unlikely to fit into memory, and we would also not want to run images one by one through the network because that is rather inefficient (and also there are other tweaks, e.g. batchnorm, that expects more than 1 trainign sample to pass through at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we use accuracy as the metric:\n",
    "def accuracy(pred, actual):\n",
    "    return (torch.argmax(pred, dim=1).int()== actual.int()).float().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine our simple model. This time, we are not adding the loss function to the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, c)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we define our loss function as the pytorch cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. run one batch through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5825, -0.1432,  0.4810,  0.2529,  0.1810, -0.4233, -0.1339,  0.0740,\n",
       "         -0.1272, -0.7218], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "model = Model(m, nh, c)\n",
    "batch = X_train[:bs, :]\n",
    "preds = model(batch)\n",
    "\n",
    "xb = X_train[0:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1406)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the model isn't trained, we would expect the model to get around 1/10 correct by pure chance\n",
    "accuracy(preds, y_train[:bs].int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculate the loss with cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3778, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the loss function is what we're calculating the gradients against to optimise the params\n",
    "loss_func(preds, y_train[:bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate the grad + update \n",
    "the whole training loop now since it's easier to see what's going on. Remeber that each gradient step, we are going in the opposite direction to the loss since we want to reduce it (hence the -ve dloss/dparam * learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n + 1)//bs + 1):\n",
    "        start_index = i * bs\n",
    "        end_index = start_index + bs\n",
    "        #forward pass\n",
    "        x = X_train[start_index: end_index, :]\n",
    "        y = y_train[start_index: end_index]\n",
    "        loss = loss_func(model(x), y)\n",
    "        #backward pass to get the grad\n",
    "        loss.backward()\n",
    "        # update the params-- set to no grad since we don't want to gradients to change while we're updating\n",
    "        # if we forget this step we'll get a `RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.` error\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    # reset the grad to zero since we don't want to accumulate gradients\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, considering that we only used a very simple network with 2 epochs' training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8190)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(X_test), y_test.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pytorch way of doing things: \n",
    "In pytorch, the variables that we update as we train the network can be fetched by a call to `model.parameters()`. Also, the alternative to using a Relu layer class is to just use F.relu from torch.functional. Let's simplify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the layer in the model just by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.weight Parameter containing:\n",
      "tensor([[ 0.0174,  0.0241, -0.0010,  ..., -0.0231,  0.0328,  0.0309],\n",
      "        [ 0.0038,  0.0221, -0.0222,  ..., -0.0221, -0.0011, -0.0122],\n",
      "        [ 0.0140,  0.0150, -0.0122,  ...,  0.0326,  0.0024,  0.0165],\n",
      "        ...,\n",
      "        [ 0.0131,  0.0238, -0.0044,  ...,  0.0031, -0.0255,  0.0249],\n",
      "        [ 0.0209,  0.0198, -0.0146,  ...,  0.0298, -0.0345, -0.0034],\n",
      "        [ 0.0103, -0.0169, -0.0052,  ..., -0.0125,  0.0308,  0.0098]],\n",
      "       requires_grad=True)\n",
      "l1.bias Parameter containing:\n",
      "tensor([-0.0153,  0.0025,  0.0105,  0.0173,  0.0316, -0.0248, -0.0113,  0.0222,\n",
      "        -0.0318,  0.0057,  0.0198,  0.0165, -0.0321,  0.0308,  0.0154,  0.0301,\n",
      "         0.0024, -0.0148,  0.0182, -0.0005,  0.0343,  0.0194,  0.0073, -0.0181,\n",
      "        -0.0298, -0.0166,  0.0087,  0.0056,  0.0100, -0.0004,  0.0228,  0.0325,\n",
      "         0.0120,  0.0163,  0.0282,  0.0356,  0.0205,  0.0130,  0.0196, -0.0246,\n",
      "        -0.0204,  0.0021, -0.0115,  0.0134, -0.0073, -0.0014,  0.0134, -0.0181,\n",
      "         0.0036,  0.0301], requires_grad=True)\n",
      "l2.weight Parameter containing:\n",
      "tensor([[-0.0738, -0.0392, -0.1161,  0.0486, -0.0377, -0.0913, -0.1305,  0.0231,\n",
      "          0.1245, -0.1279,  0.0014, -0.0903,  0.1368, -0.0167,  0.1093, -0.1153,\n",
      "          0.0272,  0.0792,  0.0269, -0.0356, -0.1179,  0.0642, -0.0342, -0.0729,\n",
      "         -0.0895,  0.0938, -0.0257, -0.1017, -0.0549,  0.0881,  0.0873,  0.1320,\n",
      "         -0.0697, -0.0158, -0.0478, -0.0788,  0.0167,  0.0287, -0.0970,  0.1024,\n",
      "         -0.0184,  0.1316,  0.0328, -0.0950,  0.0351,  0.1080, -0.0118,  0.0226,\n",
      "         -0.0645,  0.1089],\n",
      "        [ 0.0324,  0.1285, -0.0066,  0.1304,  0.0331,  0.0862, -0.0028, -0.0879,\n",
      "          0.0864,  0.0287, -0.0216, -0.1280, -0.0646,  0.0714,  0.0802, -0.0966,\n",
      "          0.0138, -0.1215,  0.1146, -0.1037, -0.0452,  0.0585,  0.0373, -0.0605,\n",
      "          0.0210, -0.0591, -0.0504, -0.0954,  0.1248,  0.0010, -0.0806,  0.1053,\n",
      "         -0.0664,  0.0131, -0.1095,  0.0619,  0.0276,  0.0680,  0.0835, -0.1222,\n",
      "         -0.0950,  0.0737, -0.0363, -0.0689, -0.0962,  0.1270, -0.1072, -0.0175,\n",
      "         -0.0809, -0.0538],\n",
      "        [-0.0620,  0.0564,  0.0414, -0.0766, -0.0391,  0.0219,  0.1346, -0.0798,\n",
      "          0.1207, -0.1210,  0.1207, -0.0723,  0.1056, -0.1336,  0.0467, -0.0793,\n",
      "          0.0232,  0.0797, -0.1026, -0.1120,  0.0685, -0.0323,  0.0159,  0.0317,\n",
      "          0.0824,  0.0034, -0.0859,  0.1408, -0.0833, -0.0218, -0.1135, -0.1115,\n",
      "          0.1121,  0.1036, -0.0206,  0.0107,  0.0763,  0.1291, -0.0564, -0.0454,\n",
      "          0.0547,  0.0047, -0.0110, -0.0911,  0.1320, -0.0378, -0.0019,  0.1383,\n",
      "         -0.0147, -0.0067],\n",
      "        [ 0.0054, -0.0271,  0.0456,  0.0463, -0.0414,  0.0012, -0.0728,  0.1401,\n",
      "         -0.0875, -0.0385, -0.0252,  0.0129,  0.1028,  0.0907, -0.1117, -0.0955,\n",
      "         -0.0467, -0.0872, -0.0170,  0.1114, -0.0595, -0.1255, -0.0484,  0.1242,\n",
      "          0.0753, -0.1240,  0.0957,  0.1385,  0.0917,  0.1044, -0.1301, -0.0304,\n",
      "          0.1044, -0.0781, -0.0013,  0.1200, -0.0233,  0.0882,  0.0214, -0.0774,\n",
      "         -0.0154,  0.0929, -0.0820, -0.0464,  0.1207, -0.0345,  0.0643, -0.0214,\n",
      "          0.1297,  0.0270],\n",
      "        [ 0.1387,  0.0800,  0.0272,  0.0307,  0.0774,  0.0693, -0.0882,  0.1329,\n",
      "         -0.0679, -0.0829,  0.0955,  0.1190, -0.0230,  0.0100,  0.1371,  0.0753,\n",
      "         -0.0702,  0.0336,  0.0808,  0.0341,  0.0356,  0.0588, -0.0327, -0.0353,\n",
      "          0.0170,  0.1270, -0.0320, -0.0710, -0.0099, -0.0162,  0.0603,  0.0324,\n",
      "          0.0961, -0.0318, -0.0928, -0.0546, -0.0464, -0.1046,  0.1265,  0.0971,\n",
      "          0.0363,  0.0790, -0.0689,  0.0744, -0.0710,  0.1044, -0.0768, -0.1116,\n",
      "         -0.1267, -0.0751],\n",
      "        [ 0.0948, -0.0238,  0.1309, -0.1356, -0.0609, -0.1380, -0.0877, -0.0856,\n",
      "         -0.0728,  0.1349,  0.0525,  0.0168,  0.1186, -0.0729,  0.0969,  0.1185,\n",
      "         -0.0039,  0.1139,  0.0685, -0.0326,  0.1208,  0.1378,  0.1136,  0.1340,\n",
      "          0.0284, -0.0315,  0.0647,  0.0385, -0.0547, -0.0486,  0.1149, -0.0962,\n",
      "         -0.0645, -0.0618,  0.0888,  0.1229,  0.1215, -0.0916,  0.1198,  0.0657,\n",
      "          0.1370,  0.0101,  0.0212, -0.0441, -0.0278, -0.0800, -0.0264,  0.0169,\n",
      "         -0.0234, -0.1323],\n",
      "        [-0.0388,  0.0388,  0.0423,  0.0005, -0.1076, -0.1215, -0.1194,  0.0305,\n",
      "          0.0914, -0.0572, -0.1171, -0.0200,  0.0388,  0.0457,  0.0656,  0.1320,\n",
      "          0.0667,  0.0415,  0.1102, -0.1142,  0.0404, -0.0469,  0.0453, -0.0697,\n",
      "         -0.0270, -0.0674,  0.0634,  0.1340,  0.0691,  0.0630,  0.0537, -0.0232,\n",
      "         -0.0959, -0.0961,  0.0824,  0.0183, -0.0747, -0.0525,  0.0233, -0.0325,\n",
      "         -0.0727,  0.1264, -0.0187,  0.0538, -0.1346,  0.0103, -0.0906, -0.0299,\n",
      "         -0.1382,  0.0046],\n",
      "        [ 0.0758,  0.0337,  0.0823,  0.0186,  0.0596,  0.0327,  0.1101,  0.0888,\n",
      "          0.1276,  0.1082, -0.0448,  0.1065, -0.0343,  0.1240, -0.0561,  0.0965,\n",
      "          0.0937, -0.0175,  0.1194,  0.0495,  0.0470,  0.0352,  0.0047, -0.0627,\n",
      "         -0.0495, -0.0763,  0.0799,  0.0801, -0.1264,  0.0621,  0.0658,  0.0013,\n",
      "          0.0166, -0.0554,  0.0244, -0.1282,  0.1056,  0.0045,  0.0391,  0.0097,\n",
      "          0.0674,  0.0049,  0.1293, -0.0979,  0.1413, -0.0563,  0.0013, -0.1121,\n",
      "         -0.0789, -0.0369],\n",
      "        [-0.0139,  0.0527, -0.0631, -0.1389, -0.0867,  0.1070, -0.1196,  0.0174,\n",
      "         -0.0986, -0.0989, -0.0157,  0.0693,  0.0588,  0.0915, -0.1231,  0.0449,\n",
      "          0.0574, -0.1258, -0.1108,  0.0836,  0.0499,  0.0353,  0.0624, -0.0161,\n",
      "          0.0638, -0.0638,  0.0960, -0.0920,  0.0822, -0.0903, -0.0954,  0.0725,\n",
      "          0.1203, -0.0071,  0.0491, -0.0123, -0.1225,  0.0700,  0.0284,  0.0130,\n",
      "          0.0300,  0.0037,  0.1259,  0.0882,  0.0338, -0.0405, -0.0988, -0.0709,\n",
      "         -0.0415, -0.0524],\n",
      "        [-0.1191,  0.1288,  0.0748, -0.0054,  0.0220,  0.0415, -0.0346,  0.0081,\n",
      "         -0.1108,  0.0227,  0.1300,  0.1023,  0.0128, -0.1353, -0.0927, -0.0266,\n",
      "          0.0655,  0.1207, -0.1156, -0.1326,  0.1300, -0.0504,  0.1284, -0.1022,\n",
      "          0.0524,  0.0387, -0.1155,  0.0669,  0.0982, -0.0243,  0.0955, -0.1361,\n",
      "          0.0026,  0.0358, -0.0166,  0.1083, -0.0789, -0.1225, -0.1183, -0.1379,\n",
      "         -0.0681,  0.0481,  0.0298,  0.0384,  0.0947, -0.0774,  0.0225,  0.1273,\n",
      "          0.0612,  0.0427]], requires_grad=True)\n",
      "l2.bias Parameter containing:\n",
      "tensor([ 0.1065, -0.0836,  0.0798, -0.0722, -0.0096,  0.0977, -0.1116,  0.1249,\n",
      "         0.1047, -0.1366], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# and to print out the trainiable params\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lr, batch_size=64):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n + 1)//batch_size + 1):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            #forward pass\n",
    "            x = X_train[start_index: end_index, :]\n",
    "            y = y_train[start_index: end_index]\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            #backward pass to get the grad\n",
    "            loss.backward()\n",
    "            # update the params-- its quite a bit simiplier since pytorch takes care fo tracking all of them down\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad* lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8042)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(2, 0.5)\n",
    "accuracy(model(X_test), y_test.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A different way of defining layers-- nn.Modulelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.ModuleList):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8339)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel([nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)])\n",
    "train(4, 0.2)\n",
    "accuracy(model(X_test), y_test.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can just use nn.Sequential instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8432)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "train(6, 0.2)\n",
    "accuracy(model(X_test), y_test.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers\n",
    "\n",
    "Since we will be calling the update step for every minibatch training, we can package this up:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p -= p.grad* lr\n",
    "    model.zero_grad()\n",
    "```                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr = 0.5):\n",
    "        self.params = list(params) #explicit cast to list in cast params is an iterator\n",
    "        self.lr = lr \n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad* self.lr\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()\n",
    "\n",
    "def train(epochs, lr, batch_size=64):\n",
    "    optimizer = Optimizer(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n + 1)//batch_size + 1):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            #forward pass\n",
    "            x = X_train[start_index: end_index, :]\n",
    "            y = y_train[start_index: end_index]\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            #backward pass to get the grad\n",
    "            loss.backward()\n",
    "            # update the params-- its quite a bit simiplier since pytorch takes care fo tracking all of them down\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8410)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "train(6, 0.2)\n",
    "accuracy(model(X_test), y_test.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can just use pytorch's `optim` instead. \n",
    "From the docs:\n",
    "\n",
    ">`torch.optim` is a package implementing various optimization algorithms.\n",
    "Most commonly used methods are already supported, and the interface is general\n",
    "enough, so that more sophisticated ones can be also easily integrated in the\n",
    "future.\n",
    "\n",
    "Usage, e.g. \n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(learning_rate):\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n",
    "    \n",
    "def train(model, epochs, optimizer, batch_size=64):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n + 1)//batch_size + 1):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            #forward pass\n",
    "            x = X_train[start_index: end_index, :]\n",
    "            y = y_train[start_index: end_index]\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            #backward pass to get the grad\n",
    "            loss.backward()\n",
    "            # update the params-- its quite a bit simiplier since pytorch takes care fo tracking all of them down\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8389)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, optimizer = generate_model(0.2)\n",
    "train(model, 6, optimizer)\n",
    "accuracy(model(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset/ Dataloader\n",
    "\n",
    "It is easier to make a mistake if we have to declare x, y batches separately every time. Also, if we have a test set, we would want to apply the same normalisation as we did to our train set, so we can combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        assert len(x) == len(y), \"size mismatch between x and y\"\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8401)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset(X_train, y_train)\n",
    "valid_ds = Dataset(X_test, y_test)\n",
    "def train(model, epochs, optimizer, batch_size=64):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n + 1)//batch_size + 1):\n",
    "\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            #forward pass\n",
    "            x, y = train_ds[start_index: end_index]\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            #backward pass to get the grad\n",
    "            loss.backward()\n",
    "            # update the params-- its quite a bit simiplier since pytorch takes care fo tracking all of them down\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "model, optimizer = generate_model(0.2)\n",
    "train(model, 6, optimizer)\n",
    "accuracy(model(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, why don't we get rid of the `for i in range((n + 1)//batch_size + 1):...` and use an generator to yiedl batches instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range((len(self.dataset) + 1)//self.batch_size + 1):\n",
    "            start_index = i * self.batch_size\n",
    "            end_index = start_index + self.batch_size\n",
    "            yield self.dataset[start_index: end_index]\n",
    "            \n",
    "def train(model, epochs, optimizer, dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:     \n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = Dataloader(train_ds, 64)\n",
    "valid_dl = Dataloader(valid_ds, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8403)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, optimizer = generate_model(0.2)\n",
    "train(model, 6, optimizer, train_dl)\n",
    "accuracy(model(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
